{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ü MANO - Colab Training Notebook\n",
    "\n",
    "Train the Colombian Sign Language gesture classifier using Google Colab's free GPU.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. **Generate preprocessed tensors** locally:\n",
    "   ```bash\n",
    "   python -m src.cv_model.preprocessing\n",
    "   ```\n",
    "   This creates `data/processed/tensors.pth`\n",
    "\n",
    "2. **Upload to Google Drive** at `My Drive/Mano_data/tensors.pth`\n",
    "\n",
    "3. **Push your code to GitHub** (for `src/` scripts)\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Mount Google Drive\n",
    "2. Clone repo ‚Üí get `src/` scripts  \n",
    "3. Load preprocessed tensors ‚Üí instant data loading!\n",
    "4. Train model with GPU acceleration\n",
    "5. Models saved to Google Drive for persistence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EkxsF2RASRf"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MANO - Colombian Sign Language Translator - Colab Training Notebook\n",
    "# =============================================================================\n",
    "# This notebook loads preprocessed tensors from Google Drive for fast training\n",
    "# =============================================================================\n",
    "\n",
    "# Install dependencies\n",
    "%pip install torch torchvision mlflow scikit-learn pillow numpy matplotlib -q\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2306,
     "status": "ok",
     "timestamp": 1764275876963,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "MuSUkl28Am26",
    "outputId": "7e519afe-5a46-4a3c-87a7-d726287497c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# ‚ö†Ô∏è CONFIGURATION - UPDATE THESE VALUES\n",
    "# =============================================================================\n",
    "REPO_URL = \"https://github.com/davidrfb/Mano.git\"  # Your GitHub repo URL\n",
    "REPO_DIR = \"/content/Mano\"\n",
    "\n",
    "# Preprocessed tensors file (generated locally with: python -m src.cv_model.preprocessing)\n",
    "TENSOR_PATH = \"/content/drive/MyDrive/Mano_data/tensors.pth\"\n",
    "\n",
    "# Where to save trained models\n",
    "MODELS_DIR = \"/content/drive/MyDrive/Mano/models\"\n",
    "# =============================================================================\n",
    "\n",
    "# Verify tensor file exists\n",
    "if os.path.exists(TENSOR_PATH):\n",
    "    size_mb = os.path.getsize(TENSOR_PATH) / 1024 / 1024\n",
    "    print(f\"‚úÖ Tensor file found: {TENSOR_PATH}\")\n",
    "    print(f\"   Size: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Tensor file NOT found at: {TENSOR_PATH}\")\n",
    "    print(\"   Please run locally: python -m src.cv_model.preprocessing\")\n",
    "    print(\"   Then upload data/processed/tensors.pth to Google Drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (or pull if already exists)\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "else:\n",
    "    print(f\"Cloning repository to {REPO_DIR}...\")\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")\n",
    "!ls -la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed tensors from Google Drive\n",
    "# This is MUCH faster than pulling individual images via DVC\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(f\"Loading tensors from {TENSOR_PATH}...\")\n",
    "data = torch.load(TENSOR_PATH, weights_only=False)\n",
    "\n",
    "# Extract data\n",
    "train_images, train_labels = data['train_images'], data['train_labels']\n",
    "val_images, val_labels = data['val_images'], data['val_labels']\n",
    "test_images, test_labels = data['test_images'], data['test_labels']\n",
    "classes = data['classes']\n",
    "num_classes = data['num_classes']\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Train: {train_images.shape} ({len(train_labels)} samples)\")\n",
    "print(f\"   Val: {val_images.shape} ({len(val_labels)} samples)\")\n",
    "print(f\"   Test: {test_images.shape} ({len(test_labels)} samples)\")\n",
    "print(f\"   Classes ({num_classes}): {classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training (applied on-the-fly to normalized tensors)\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "class AugmentedTensorDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that applies augmentation to pre-normalized tensors.\"\"\"\n",
    "    def __init__(self, images, labels, augment=False):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Augmentation for normalized tensors (careful with intensity)\n",
    "        self.transforms = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.3),\n",
    "            T.RandomRotation(degrees=15),\n",
    "            T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            # ColorJitter works on normalized tensors too\n",
    "            T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
    "        ]) if augment else None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.augment and self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ Augmentation pipeline ready\")\n",
    "print(\"   Train augmentations: RandomHorizontalFlip, RandomRotation, RandomAffine, ColorJitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Add repo to path for model imports\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "from src.cv_model.train import get_model, train_one_epoch, evaluate\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Models will be saved to: {MODELS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1764275876971,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "n8cakbSWAzgS"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER SEARCH CONFIGURATION\n",
    "# =============================================================================\n",
    "MODELS_TO_TRAIN = [\"mobilenet_v2\", \"mobilenet_v3_small\", \"efficientnet_b0\", \"resnet18\"]\n",
    "\n",
    "# Learning rates to search (1e-3 already done, skip it)\n",
    "LEARNING_RATES = [5e-4, 3e-3, 5e-3]  # Excluding 1e-3 which was already run\n",
    "\n",
    "# Batch sizes to try\n",
    "BATCH_SIZES = [32, 64]\n",
    "\n",
    "# Fixed hyperparameters\n",
    "EPOCHS = 30\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 10\n",
    "EXPERIMENT_NAME = \"V2_moredata\"\n",
    "\n",
    "# Calculate total runs\n",
    "total_runs = len(MODELS_TO_TRAIN) * len(LEARNING_RATES) * len(BATCH_SIZES)\n",
    "print(f\"=\" * 60)\n",
    "print(f\"HYPERPARAMETER SEARCH\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"Learning rates: {LEARNING_RATES}\")\n",
    "print(f\"Batch sizes: {BATCH_SIZES}\")\n",
    "print(f\"Total experiments: {total_runs}\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# MLflow setup\n",
    "MLFLOW_TRACKING_URI = f\"file://{MODELS_DIR}/mlruns\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(f\"MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1764275876996,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "v6F5FuAsCBHK"
   },
   "outputs": [],
   "source": [
    "# Store results for all experiments\n",
    "all_results = []\n",
    "run_count = 0\n",
    "\n",
    "# Hyperparameter search: iterate over all combinations\n",
    "for MODEL_NAME in MODELS_TO_TRAIN:\n",
    "    for LEARNING_RATE in LEARNING_RATES:\n",
    "        for BATCH_SIZE in BATCH_SIZES:\n",
    "            run_count += 1\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(f\"üöÄ EXPERIMENT {run_count}/{total_runs}\")\n",
    "            print(f\"   Model: {MODEL_NAME} | LR: {LEARNING_RATE} | Batch: {BATCH_SIZE}\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            # Create DataLoaders with current batch size and augmentation\n",
    "            train_dataset = AugmentedTensorDataset(train_images, train_labels, augment=True)\n",
    "            val_dataset = AugmentedTensorDataset(val_images, val_labels, augment=False)\n",
    "            test_dataset = AugmentedTensorDataset(test_images, test_labels, augment=False)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "            \n",
    "            # Create fresh model\n",
    "            print(f\"Initializing {MODEL_NAME} with pretrained weights...\")\n",
    "            model = get_model(MODEL_NAME, num_classes, pretrained=True)\n",
    "            model = model.to(DEVICE)\n",
    "            \n",
    "            # Count parameters\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(f\"Total parameters: {total_params:,}\")\n",
    "            \n",
    "            # Fresh optimizer and scheduler\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE / 100)\n",
    "            \n",
    "            # Training loop\n",
    "            run_name = f\"{MODEL_NAME}_lr{LEARNING_RATE}_bs{BATCH_SIZE}\"\n",
    "            best_val_loss = float('inf')  # Track loss (lower is better)\n",
    "            best_val_acc = 0.0  # Still track for reporting\n",
    "            epochs_without_improvement = 0\n",
    "            best_checkpoint_path = None\n",
    "            \n",
    "            with mlflow.start_run(run_name=run_name):\n",
    "                # Log parameters\n",
    "                mlflow.log_params({\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"epochs\": EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"weight_decay\": WEIGHT_DECAY,\n",
    "                    \"patience\": PATIENCE,\n",
    "                    \"num_classes\": num_classes,\n",
    "                    \"classes\": \",\".join(classes),\n",
    "                    \"optimizer\": \"AdamW\",\n",
    "                    \"scheduler\": \"CosineAnnealingLR\",\n",
    "                    \"device\": str(DEVICE),\n",
    "                    \"pretrained\": True,\n",
    "                    \"augmentation\": True,\n",
    "                    \"train_samples\": len(train_loader.dataset),\n",
    "                    \"val_samples\": len(val_loader.dataset),\n",
    "                    \"test_samples\": len(test_loader.dataset),\n",
    "                    \"total_params\": total_params,\n",
    "                    \"trainable_params\": trainable_params,\n",
    "                })\n",
    "\n",
    "                print(\"-\" * 60)\n",
    "                print(f\"Training with augmentation enabled...\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "                for epoch in range(1, EPOCHS + 1):\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Train\n",
    "                    train_loss, train_acc = train_one_epoch(\n",
    "                        model, train_loader, criterion, optimizer, DEVICE\n",
    "                    )\n",
    "\n",
    "                    # Validate\n",
    "                    val_loss, val_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "                    # Update scheduler\n",
    "                    scheduler.step()\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "                    # Log metrics to MLflow\n",
    "                    mlflow.log_metrics({\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"train_acc\": train_acc,\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"val_acc\": val_acc,\n",
    "                        \"learning_rate\": current_lr,\n",
    "                    }, step=epoch)\n",
    "\n",
    "                    # Logging\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(\n",
    "                        f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "                        f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                        f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
    "                        f\"LR: {current_lr:.6f} | \"\n",
    "                        f\"Time: {elapsed:.1f}s\"\n",
    "                    )\n",
    "\n",
    "                    # Save best model (based on validation LOSS - lower is better)\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_val_acc = val_acc  # Track best acc at best loss\n",
    "                        epochs_without_improvement = 0\n",
    "\n",
    "                        # Save checkpoint\n",
    "                        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                        filename = f\"{MODEL_NAME}_lr{LEARNING_RATE}_bs{BATCH_SIZE}_acc{val_acc:.2f}.pth\"\n",
    "                        filepath = Path(MODELS_DIR) / filename\n",
    "\n",
    "                        checkpoint = {\n",
    "                            \"model_state_dict\": model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                            \"epoch\": epoch,\n",
    "                            \"val_loss\": val_loss,\n",
    "                            \"val_acc\": val_acc,\n",
    "                            \"model_name\": MODEL_NAME,\n",
    "                            \"learning_rate\": LEARNING_RATE,\n",
    "                            \"batch_size\": BATCH_SIZE,\n",
    "                            \"classes\": classes,\n",
    "                            \"num_classes\": num_classes,\n",
    "                        }\n",
    "                        torch.save(checkpoint, filepath)\n",
    "\n",
    "                        # Log to MLflow\n",
    "                        mlflow.log_artifact(str(filepath), artifact_path=\"checkpoints\")\n",
    "                        best_checkpoint_path = filepath\n",
    "                        print(f\"  ‚Ü≥ New best! (loss: {val_loss:.4f})\")\n",
    "                    else:\n",
    "                        epochs_without_improvement += 1\n",
    "\n",
    "                    # Early stopping (based on validation loss)\n",
    "                    if epochs_without_improvement >= PATIENCE:\n",
    "                        print(f\"\\nEarly stopping at epoch {epoch} (val_loss not improving)\")\n",
    "                        mlflow.log_param(\"early_stopped_epoch\", epoch)\n",
    "                        break\n",
    "\n",
    "                # Final evaluation on test set\n",
    "                print(\"\\nEvaluating on test set...\")\n",
    "                test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
    "                print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "                # Log final metrics\n",
    "                mlflow.log_metrics({\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"best_val_acc\": best_val_acc,\n",
    "                    \"test_loss\": test_loss,\n",
    "                    \"test_acc\": test_acc,\n",
    "                })\n",
    "\n",
    "                # Register best model in MLflow model registry\n",
    "                if best_checkpoint_path:\n",
    "                    mlflow.pytorch.log_model(\n",
    "                        model,\n",
    "                        artifact_path=\"model\",\n",
    "                        registered_model_name=f\"lsc_{MODEL_NAME}\",\n",
    "                    )\n",
    "\n",
    "                run_id = mlflow.active_run().info.run_id\n",
    "                \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"lr\": LEARNING_RATE,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"best_val_acc\": best_val_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"params\": total_params,\n",
    "                \"run_id\": run_id,\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n‚úÖ Complete! Val Loss: {best_val_loss:.4f}, Val Acc: {best_val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            del model, optimizer, scheduler, train_loader, val_loader, test_loader\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üèÅ HYPERPARAMETER SEARCH COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1764277736114,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "GspiZJxuAzjT",
    "outputId": "63846288-04da-45a3-ace4-6e485808c4f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/27 21:08:55 INFO mlflow.tracking.fluent: Experiment with name 'colab_mobilenet_v3_small' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "MLflow tracking URI: file:///content/drive/MyDrive/Mano/models/mlruns\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison of all experiments\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä HYPERPARAMETER SEARCH RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "# Sort by best_val_loss (ascending - lower is better)\n",
    "results_df = results_df.sort_values(\"best_val_loss\", ascending=True)\n",
    "\n",
    "# Format for display\n",
    "display_df = results_df.copy()\n",
    "display_df['lr'] = display_df['lr'].apply(lambda x: f\"{x:.0e}\")\n",
    "display_df['best_val_loss'] = display_df['best_val_loss'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['best_val_acc'] = display_df['best_val_acc'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['test_acc'] = display_df['test_acc'].apply(lambda x: f\"{x:.4f}\")\n",
    "print(display_df[['model', 'lr', 'batch_size', 'best_val_loss', 'best_val_acc', 'test_acc']].to_string(index=False))\n",
    "\n",
    "# Best configuration (lowest val_loss)\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION (by val_loss):\")\n",
    "print(f\"   Model: {best['model']}\")\n",
    "print(f\"   Learning Rate: {best['lr']}\")\n",
    "print(f\"   Batch Size: {best['batch_size']}\")\n",
    "print(f\"   Val Loss: {best['best_val_loss']:.4f}\")\n",
    "print(f\"   Val Accuracy: {best['best_val_acc']:.4f}\")\n",
    "print(f\"   Test Accuracy: {best['test_acc']:.4f}\")\n",
    "\n",
    "# Best per model\n",
    "print(f\"\\nüìà BEST LR/BATCH PER MODEL (by val_loss):\")\n",
    "for model in MODELS_TO_TRAIN:\n",
    "    model_results = results_df[results_df['model'] == model]\n",
    "    if len(model_results) > 0:\n",
    "        best_for_model = model_results.iloc[0]\n",
    "        print(f\"   {model}: LR={best_for_model['lr']:.0e}, BS={best_for_model['batch_size']}, Loss={best_for_model['best_val_loss']:.4f}, Acc={best_for_model['test_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1764277737575,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "7MbCgw5bAzma",
    "outputId": "d1204dcd-c6d7-45d2-a911-73feae1b799e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 1871 images from 26 classes\n",
      "Split sizes - Train: 1309, Val: 281, Test: 281\n",
      "Loaded 1871 images from 26 classes\n",
      "Loaded 1871 images from 26 classes\n",
      "Loaded 1871 images from 26 classes\n",
      "Classes (26): ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Train batches: 41\n",
      "Val batches: 9\n",
      "Test batches: 9\n"
     ]
    }
   ],
   "source": [
    "# View all MLflow runs for this experiment\n",
    "print(f\"MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"To view results locally: mlflow ui --backend-store-uri {MLFLOW_TRACKING_URI.replace('file://', '')}\")\n",
    "\n",
    "# List all runs from this experiment\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "if experiment:\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    print(f\"\\nüìã All runs in '{EXPERIMENT_NAME}' ({len(runs)} total):\")\n",
    "    cols = ['params.model_name', 'params.learning_rate', 'params.batch_size', \n",
    "            'metrics.best_val_loss', 'metrics.best_val_acc', 'metrics.test_acc', 'status']\n",
    "    available_cols = [c for c in cols if c in runs.columns]\n",
    "    if available_cols:\n",
    "        display_runs = runs[available_cols].copy()\n",
    "        display_runs.columns = [c.split('.')[-1] for c in display_runs.columns]\n",
    "        # Sort by best_val_loss if available\n",
    "        if 'best_val_loss' in display_runs.columns:\n",
    "            display_runs = display_runs.sort_values('best_val_loss', ascending=True)\n",
    "        print(display_runs.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307019,
     "status": "ok",
     "timestamp": 1764278044618,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "CcxVuTJ2Azo-",
    "outputId": "9019e135-073e-4561-ec39-496510a3c328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing mobilenet_v3_small with pretrained weights...\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.83M/9.83M [00:00<00:00, 151MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,544,506\n",
      "Trainable parameters: 1,544,506\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "Epoch   1/30 | Train Loss: 0.9120 Acc: 0.7762 | Val Loss: 0.2520 Acc: 0.9075 | LR: 0.000997 | Time: 14.7s\n",
      "  ‚Ü≥ New best! Saved to mobilenet_v3_small_v1_acc0.91_20251127_210912.pth\n",
      "Epoch   2/30 | Train Loss: 0.0408 Acc: 0.9893 | Val Loss: 0.3104 Acc: 0.8897 | LR: 0.000989 | Time: 15.9s\n",
      "Epoch   3/30 | Train Loss: 0.0217 Acc: 0.9954 | Val Loss: 0.2389 Acc: 0.9217 | LR: 0.000976 | Time: 14.4s\n",
      "  ‚Ü≥ New best! Saved to mobilenet_v3_small_v1_acc0.92_20251127_210942.pth\n",
      "Epoch   4/30 | Train Loss: 0.0201 Acc: 0.9947 | Val Loss: 0.3572 Acc: 0.8790 | LR: 0.000957 | Time: 14.9s\n",
      "Epoch   5/30 | Train Loss: 0.0137 Acc: 0.9954 | Val Loss: 0.1606 Acc: 0.9609 | LR: 0.000934 | Time: 14.4s\n",
      "  ‚Ü≥ New best! Saved to mobilenet_v3_small_v1_acc0.96_20251127_211012.pth\n",
      "Epoch   6/30 | Train Loss: 0.0248 Acc: 0.9916 | Val Loss: 0.0758 Acc: 0.9751 | LR: 0.000905 | Time: 14.5s\n",
      "  ‚Ü≥ New best! Saved to mobilenet_v3_small_v1_acc0.98_20251127_211027.pth\n",
      "Epoch   7/30 | Train Loss: 0.0267 Acc: 0.9954 | Val Loss: 1.1848 Acc: 0.7722 | LR: 0.000873 | Time: 14.5s\n",
      "Epoch   8/30 | Train Loss: 0.0199 Acc: 0.9962 | Val Loss: 0.0485 Acc: 0.9751 | LR: 0.000836 | Time: 14.2s\n",
      "Epoch   9/30 | Train Loss: 0.0073 Acc: 0.9977 | Val Loss: 0.0074 Acc: 0.9929 | LR: 0.000796 | Time: 14.5s\n",
      "  ‚Ü≥ New best! Saved to mobilenet_v3_small_v1_acc0.99_20251127_211110.pth\n",
      "Epoch  10/30 | Train Loss: 0.0015 Acc: 1.0000 | Val Loss: 0.0002 Acc: 1.0000 | LR: 0.000753 | Time: 14.3s\n",
      "  ‚Ü≥ New best! Saved to mobilenet_v3_small_v1_acc1.00_20251127_211125.pth\n",
      "Epoch  11/30 | Train Loss: 0.0009 Acc: 1.0000 | Val Loss: 0.0001 Acc: 1.0000 | LR: 0.000706 | Time: 16.3s\n",
      "Epoch  12/30 | Train Loss: 0.0005 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000658 | Time: 14.5s\n",
      "Epoch  13/30 | Train Loss: 0.0018 Acc: 0.9992 | Val Loss: 0.0001 Acc: 1.0000 | LR: 0.000608 | Time: 14.6s\n",
      "Epoch  14/30 | Train Loss: 0.0022 Acc: 0.9992 | Val Loss: 0.0001 Acc: 1.0000 | LR: 0.000557 | Time: 14.3s\n",
      "Epoch  15/30 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000505 | Time: 14.6s\n",
      "Epoch  16/30 | Train Loss: 0.0005 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000453 | Time: 14.2s\n",
      "Epoch  17/30 | Train Loss: 0.0005 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000402 | Time: 14.5s\n",
      "Epoch  18/30 | Train Loss: 0.0006 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000352 | Time: 17.0s\n",
      "Epoch  19/30 | Train Loss: 0.0010 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000304 | Time: 14.2s\n",
      "Epoch  20/30 | Train Loss: 0.0002 Acc: 1.0000 | Val Loss: 0.0000 Acc: 1.0000 | LR: 0.000258 | Time: 14.3s\n",
      "\n",
      "Early stopping at epoch 20 (no improvement for 10 epochs)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/27 21:13:55 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000 | Test Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/27 21:13:56 WARNING mlflow.utils.requirements_utils: Found torch version (2.9.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torch==2.9.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/11/27 21:14:03 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.24.0+cu126) contains a local version label (+cu126). MLflow logged a pip requirement for this package as 'torchvision==0.24.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[31m2025/11/27 21:14:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Successfully registered model 'lsc_mobilenet_v3_small'.\n",
      "Created version '1' of model 'lsc_mobilenet_v3_small'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training complete! Best validation accuracy: 1.0000\n",
      "Test accuracy: 1.0000\n",
      "Models saved to: /content/drive/MyDrive/Mano/models\n",
      "MLflow run ID: d4f96492d7224d4ba0f75676c44bcaca\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1764278044804,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "YJ7VCl9zAzrx",
    "outputId": "bab57968-2a44-4800-93f0-f6ca274be78f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file:///content/drive/MyDrive/Mano/models/mlruns\n",
      "To view results, download the folder: /content/drive/MyDrive/Mano/models/mlruns\n",
      "Or run: mlflow ui --backend-store-uri /content/drive/MyDrive/Mano/models/mlruns\n",
      "\n",
      "Recent runs:\n",
      "                             run_id  metrics.val_acc  metrics.test_acc  \\\n",
      "0  d4f96492d7224d4ba0f75676c44bcaca              1.0               1.0   \n",
      "\n",
      "     status  \n",
      "0  FINISHED  \n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1764278044806,
     "user": {
      "displayName": "David Figueroa",
      "userId": "11545643378950321568"
     },
     "user_tz": -60
    },
    "id": "gvnInWE6Enty"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM6XhiG5h5UlsyQTMqyL0Eb",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
